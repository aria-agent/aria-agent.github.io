<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Training Language Agents with Intention-Driven Reward Aggregation">
  <meta name="keywords" content="ARIA, Language Agents, Reinforcement Learning, NLP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARIA: Training Language Agents with Intention-Driven Reward Aggregation</title>

  <link rel="icon" href="static/images/aria_icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>
  
  <!-- MathJax for rendering LaTeX -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <style>
    pre {
        max-height: 400px; 
        overflow-x: auto; 
        overflow-y: auto;
        background-color: #f0f0f0; 
        padding: 10px;
        border: 1px solid #ccc; 
        text-align: left;
    }
    
    @keyframes float {
      0%, 100% { transform: translateY(0px); }
      50% { transform: translateY(-10px); }
    }
    
    @keyframes pulse {
      0%, 100% { opacity: 0.4; transform: scale(1); }
      50% { opacity: 0.8; transform: scale(1.1); }
    }
    
    .floating-dot {
      animation: float 3s ease-in-out infinite;
    }
    
    .pulsing-dot {
      animation: pulse 2s ease-in-out infinite;
    }
    
    .floating-dot:nth-child(2) { animation-delay: 0.5s; }
    .floating-dot:nth-child(3) { animation-delay: 1s; }
    .floating-dot:nth-child(4) { animation-delay: 1.5s; }
    .floating-dot:nth-child(5) { animation-delay: 2s; }
    .floating-dot:nth-child(6) { animation-delay: 2.5s; }
    .floating-dot:nth-child(7) { animation-delay: 3s; }
</style>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://selfgoal-agent.github.io/">
              <b>SELFGOAL🔥</b>
            </a>
            <a class="navbar-item" href="https://time-arena.github.io/">
              <b>TimeArena🔥</b>
            </a>
            <a class="navbar-item" href="https://auction-arena.github.io/">
                                <b>AuctionArena🔥</b>
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="title is-1 publication-title is-bold" style="display: flex; align-items: center; justify-content: center; margin-bottom: 2rem;">
              <svg width="640" height="220" viewBox="0 0 800 280" xmlns="http://www.w3.org/2000/svg" style="margin-right: 0;">
                <defs>
                  <linearGradient id="blueGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                    <stop offset="0%" style="stop-color:#1E90FF;stop-opacity:1" />
                    <stop offset="33%" style="stop-color:#4169E1;stop-opacity:1" />
                    <stop offset="66%" style="stop-color:#6495ED;stop-opacity:1" />
                    <stop offset="100%" style="stop-color:#87CEEB;stop-opacity:1" />
                  </linearGradient>
                  <linearGradient id="blueGradient_sec" x1="0%" y1="0%" x2="100%" y2="0%">
                    <stop offset="0%" style="stop-color:#74B9FF;stop-opacity:1" />
                    <stop offset="100%" style="stop-color:#0984E3;stop-opacity:1" />
                  </linearGradient>
                  <filter id="blue_shadow">
                    <feDropShadow dx="4" dy="4" stdDeviation="6" flood-opacity="0.4"/>
                  </filter>
                </defs>
                
                <!-- ARIA文字 - 更大更突出 -->
                <text x="400" y="140" font-family="Inter, Arial, sans-serif" font-size="108" font-weight="bold" fill="url(#blueGradient)" filter="url(#blue_shadow)" text-anchor="middle">ARIA</text>
                
                <!-- 右边的装饰元素 - 放大 -->
                <g class="floating-decorations">
                  <circle cx="680" cy="50" r="8" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                  <circle cx="720" cy="80" r="6" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.5"/>
                  <circle cx="700" cy="110" r="5" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.7"/>
                  <circle cx="750" cy="40" r="4" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.4"/>
                  <circle cx="730" cy="130" r="3" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                  <circle cx="760" cy="100" r="7" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.5"/>
                  <circle cx="690" cy="140" r="5" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                </g>
                
                <!-- 副标题 -->
                <text x="400" y="180" font-family="Inter, Arial, sans-serif" font-size="20" font-weight="500" fill="url(#blueGradient)" opacity="0.8" text-anchor="middle">INTENTION-DRIVEN REWARD AGGREGATION</text>
              </svg>
            </div>
            <h2 class="subtitle is-2 publication-subtitle" style="font-size: 2rem; margin-top: 1rem;">
              Training Language Agents with Intention-Driven Reward Aggregation
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=asTSVwQAAAAJ&hl=zh-CN">Ruihan Yang*</a><sup style="color:#6fbf73;">1</sup>,</span>
              <span class="author-block">
                <a href="https://ykzhang721.github.io/">Yikai Zhang*</a><sup style="color:#6fbf73;">1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=FAJzMAQAAAAJ&hl=zh-CN">Aili Chen</a><sup style="color:#6fbf73;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://neph0s.github.io/">Xintao Wang</a><sup style="color:#6fbf73">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://siyuyuan.github.io/">Siyu Yuan</a><sup style="color:#6fbf73">1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://jiangjiechen.github.io/">Jiangjie Chen</a><sup style="color:#ed4b82;">2</sup>,
              </span>
              <span class="author-block">
                <a href="#">Deqing Yang</a><sup style="color:#6fbf73;">1†</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=odFW4FoAAAAJ">Yanghua Xiao</a><sup style="color:#6fbf73;">1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>Fudan University</span>
              <span class="author-block"><sup style="color:#ed4b82">2</sup>Bytedance Seed</span><br>
              <span class="author-block">* Equal Contribution</span><br>
              <span class="author-block">† Corresponding author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="static/papers/aria_paper.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">🤗</p>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Twitter Link. -->
                <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-x-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="static/images/aria_overview.png" alt="ARIA Overview" width="100%" />
        <p>Overview of ARIA. ARIA first lets agents interact to collect trajectories, then performs semantic projection and aggregates rewards in the intention space, and finally updates the policy using the aggregated rewards.
        </p>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL).
            </p>
            <p>
              To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization.
            </p>
            <p>
              Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average 9.95% across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- METHOD SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">ARIA Method</h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Reward Aggregation</h2>
          <div class="content has-text-justified">
            <p>
              Let $c(\tau) = \{c_k(a_1), c_k(o_1), \ldots, c_k(a_T), c_k(o_T)\}$ denote the labeled sequence of trajectory $\tau$ and $R$ denote the reward. First, we have the return at $t$ step as discounted reward with coefficient $\gamma$:
            </p>
            <div class="has-text-centered">
              $$R(h_t, a_t) = \gamma^{T-t} R$$
            </div>
            <p>
              Then, we calculate the aggregated return as:
            </p>
            <div class="has-text-centered">
              $$\tilde{R}(\tilde{h}_t, \tilde{a}_t) = \frac{1}{|\mathcal{D}_{\tilde{h}_t, \tilde{a}_t}|} \sum_{\tau \in \mathcal{D}_{\tilde{h}_t, \tilde{a}_t}} R(\tilde{h}_t, \tilde{a}_t)$$
            </div>
            <p>
              where $\mathcal{D}_{\tilde{h}_t, \tilde{a}_t}$ denotes all trajectories with $(\tilde{h}_t, \tilde{a}_t)$ as prefix in the intention sequences. Finally, we assign $\tilde{R}(\tilde{h}_t, \tilde{a}_t)$ to $(h_t, a_t)$ and use it as advantage $\tilde{A}(h_t,a_t)$ for optimization.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Reward-Oriented Granularity Selection For Clustering</h2>
          <div class="content has-text-justified">
            <p>
              Traditional clustering metrics tend to favor overly coarse groupings due to high similarity among actions, overlooking fine-grained distinctions critical for RL tasks. We propose a reward-oriented granularity selection mechanism that assesses whether further splitting clusters yields meaningful reward change.
            </p>
            <p>
              <strong>SplitScore:</strong> Let $k \in [2, K]$ denote all possible granularity levels. We use SplitScore to select the optimal granularity $k^{*}$, defined as:
            </p>
            <div class="has-text-centered">
              $$\text{SplitScore}(k) = \frac{\delta_k}{|\mathcal{D}|}$$
            </div>
            <p>
              where $\delta_k = \left| \tilde{R}^{(k+1)}(h_t, a_t) - \tilde{R}^{(k)}(h_t, a_t) \right|$ represents the reward change when the number of clusters changes from $k$ to $k+1$, and $\mathcal{D}$ is the collection of all $(h_t, a_t)$ pairs.
            </p>
            <p>
              <strong>Automatic Stopping Criterion:</strong> Given a threshold $\epsilon > 0$ and window size $\tau$, we stop splitting when $\text{SplitScore}(j) < \epsilon$ for all $j \in [k, k + \tau]$ as $k$ increases. The selected $k$ is then taken as $k^*$.
            </p>
          </div>
        </div>
      </div>
    
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Offline REINFORCE with Aggregated Reward</h2>
          <div class="content has-text-justified">
            <p>
              We use the offline REINFORCE algorithm to optimize the policy. Formally, let $\pi_\theta(a \mid s)$ denote the policy parameterized by $\theta$ and assign the aggregated reward $\tilde{R}^{(k)}(h_t,a_t)$ to $\tilde{A}(h_t,a_t)$. ARIA optimizes the model by maximizing the following objective:
            </p>
            <div class="has-text-centered">
              $$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T  \log \pi_\theta(a_t \mid h_t) \cdot \tilde{A}(h_t,a_t) \right]$$
            </div>
            <p>
              This objective leverages the variance-reduced advantages obtained through intention-aware reward aggregation, enabling more stable and efficient policy updates compared to standard REINFORCE with sparse rewards.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">Experimental Results</h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Main Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main_results_adversarial.png" alt="Main Results Adversarial" height="9%" />
                <p>Main results on adversarial games. ARIA achieves significant improvements of 9.67% and 9.83% on Bargaining and Negotiation tasks respectively, consistently outperforming strong offline and online RL baselines.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main_results_single.png" alt="Main Results Single" width="45%" />
                <p>
                  Main results on single-agent games. ARIA outperforms all baselines by an average of 9.82% on Twenty Questions and Guess My City tasks, demonstrating its effectiveness across different task types.
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/online_results.png" alt="Online Results" width="75%" />
                <p>Online ARIA results showing faster convergence and higher returns compared to existing online methods (ArCHer and RAGEN) across iterations.</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Analysis</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/reward_variance.png" alt="Reward Aggregation Significantly Reduces Reward Variance" width="82%" />
                <p><strong>Reward Aggregation Significantly Reduces Reward Variance:</strong> Our analysis demonstrates that reward aggregation markedly reduces the fluctuation range of action rewards compared to binary reward distributions that are highly polarized with values concentrated near 0 or 1. In large action spaces where most actions are sampled only once, the original binary rewards result in high variance. Through reward aggregation, actions within the same semantic cluster share a common reward, which significantly smooths the distribution and reduces variance across all tasks, highlighting the effectiveness and necessity of this approach in stabilizing policy learning.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/optimize_policy.png" alt="Reward Aggregation Improves Policy Optimization" width="82%" />
                <p><strong>Reward Aggregation Improves Policy Optimization:</strong> The policy loss curves reveal that ARIA, which applies semantic-level reward aggregation, accelerates loss reduction compared to vanilla REINFORCE baseline, providing stronger learning signals and enabling faster policy updates with improved sample efficiency. Despite converging to similar loss levels, ARIA outperforms other variants by 17.91% and 13.80% on bargaining and negotiation tasks respectively. We attribute these gains to the complementary effects of reward decay (introducing temporal structure for credit assignment) and reward aggregation (substantially lowering reward variance through shared signals for semantically similar actions), resulting in improved gradient estimation quality.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/generalization_results.png" alt="Generalization to Other Models" width="55%" />
                <p><strong>Generalization to Other Models:</strong> To assess the transferability of ARIA, we applied it to Qwen models (Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct) and observed consistent improvements across different base models. This demonstrates that our reward aggregation approach is model-agnostic and independent of specific architectural features or pretraining data of the underlying language models. We attribute this generalizability to the shared structural properties in the semantic spaces learned by large-scale language models, allowing ARIA to leverage these commonalities while preserving task-specific discriminative signals.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
  </section>

  <!-- THEORETICAL ANALYSIS SECTION -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Theoretical Analysis</h2>
          <div class="content has-text-justified">
            <p>
              We provide theoretical analysis showing that intention clustering-based aggregation reduces the variance of gradient descent while maintaining a small bound of bias, thus improving training stability and efficiency.
            </p>
            <p>
              <strong>Variance Reduction:</strong> By replacing original advantages $A$ with cluster-averaged advantages $\tilde{A}$, we remove the intra-cluster variance $\mathbb{E}[\text{Var}(A | C)]$, lowering the total variance of the policy gradient estimate: $\text{Var}(\tilde{A}) \leq \text{Var}(A)$.
            </p>
            <p>
              <strong>Bounded Bias:</strong> Through $\epsilon$-bisimulation analysis, we show that the bias introduced by reward aggregation is bounded: $|\mathbb{E}[\nabla_\theta \log \pi_\theta(a | h)(A(h, a) - \tilde{A}(h, a))]| \leq O(\epsilon)$, where actions within each cluster are $\epsilon$-bisimilar.
            </p>
            <p>
              <strong>Convergence Improvement:</strong> The variance reduction leads to faster convergence with fewer samples: $\|\hat{g} - g\|_2 = O(\sqrt{\sigma/N})$, where $\sigma$ is reduced through clustering.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- CASE STUDY SECTION -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Case Studies</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-left">
                <h4><strong>Twenty Questions Example</strong></h4>
                <pre><code>Actor: Is it a living thing? Oracle: Yes.
Actor: Is it a mammal? Oracle: Yes.
Actor: Is it a human? Oracle: No.
Actor: Is it a carnivore? Oracle: No.
Actor: Is it a cow? Oracle: Yes.</code></pre>
                <p style="text-align: center;">Agent successfully identifies the target object within 5 turns using strategic questioning.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-left">
                <h4><strong>Bargaining Example</strong></h4>
                <pre><code>Task Description:
In this game, there are two players. You are playing as Bob. You and Alice need to divide 100$ between you. In each round of the game, one of the players will make a proposal for division and can describe their proposal in a few words. The other player will decide whether to accept the offer and finish the game, or to proceed to the next round, in which they will be the one making the proposal. Your goal in this game is to achieve more money than Alice. Beware of inflation! With each passing round, the money is worth 0% less for you. For Alice, the money is worth 20% less. You have 20 rounds to divide the money, or both of you will get nothing!
Alice: I'm starting strong, let's start with 60-40 split!
Bob: I propose a more balanced split, considering the inflation effect on your side. This offer is fair and beneficial for both of us. What about a 55-45 split?
Alice: I'm willing to compromise, but I want a better share. I propose 70-30 Split!
Bob: Alice, consider that with each round, inflation affects your gains more than mine. This 60-40 split is actually more favorable to you than it appears due to the inflation factor. It's in both our interests to reach an agreement soon.
Alice: I'm not willing to accept a 60-40 split, even with the inflation factor. I'll try to negotiate a better deal. What about 65-35 Split?
Bob: Alice, let's be practical. With the current inflation rate, your $42 is actually worth about $17.15 in original value, while my $58 remains at $58. This split gives us both a fair share considering the inflation impact. We should agree soon to minimize your losses due to inflation.
Alice: I accept with the 42-58 split.
              </code></pre>
                <p style="text-align: center;">Agent demonstrates strategic negotiation skills considering game dynamics and inflation effects.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX SECTION -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@article{yang2024aria,
  title={ARIA: Training Language Agents with Intention-Driven Reward Aggregation},
  author={Yang, Ruihan and Zhang, Yikai and Chen, Aili and Wang, Xintao and Yuan, Siyu and Chen, Jiangjie and Yang, Deqing and Xiao, Yanghua},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
  </section>

  <section>
    <div class="section" id="org-banners" style="display:flex">
      <a href="https://www.fudan.edu.cn/en/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/fdu.png" alt="Fudan University">
      </a>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a
              rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
