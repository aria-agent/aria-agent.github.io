<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Training Language Agents with Intention-Driven Reward Aggregation">
  <meta name="keywords" content="ARIA, Language Agents, Reinforcement Learning, NLP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARIA: Training Language Agents with Intention-Driven Reward Aggregation</title>

  <link rel="icon" href="static/images/aria_icon.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>
  
  <!-- MathJax for rendering LaTeX -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <style>
    body {
      background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
      min-height: 100vh;
    }
    
    pre {
        max-height: 400px; 
        overflow-x: auto; 
        overflow-y: auto;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 15px;
        border: 1px solid #dee2e6;
        border-radius: 8px;
        text-align: left;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    @keyframes float {
      0%, 100% { transform: translateY(0px); }
      50% { transform: translateY(-10px); }
    }
    
    @keyframes pulse {
      0%, 100% { opacity: 0.4; transform: scale(1); }
      50% { opacity: 0.8; transform: scale(1.1); }
    }
    
    .floating-dot {
      animation: float 3s ease-in-out infinite;
    }
    
    .pulsing-dot {
      animation: pulse 2s ease-in-out infinite;
    }
    
    .floating-dot:nth-child(2) { animation-delay: 0.5s; }
    .floating-dot:nth-child(3) { animation-delay: 1s; }
    .floating-dot:nth-child(4) { animation-delay: 1.5s; }
    .floating-dot:nth-child(5) { animation-delay: 2s; }
    .floating-dot:nth-child(6) { animation-delay: 2.5s; }
    .floating-dot:nth-child(7) { animation-delay: 3s; }
    
    /* Enhanced styling */
    .hero {
      background: transparent;
    }
    
    .hero.is-light {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      position: relative;
      overflow: hidden;
    }
    
    .hero.is-light::before {
      content: "";
      position: absolute;
      top: -50%;
      left: -50%;
      width: 200%;
      height: 200%;
      background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
      animation: rotate 30s linear infinite;
    }
    
    @keyframes rotate {
      from { transform: rotate(0deg); }
      to { transform: rotate(360deg); }
    }
    
    .hero.is-light .title {
      color: white !important;
      text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
    }
    
    .section {
      background: transparent;
      padding: 3rem 1.5rem;
    }
    
    .box {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0,0,0,0.1);
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255,255,255,0.3);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .box:hover {
      transform: translateY(-5px);
      box-shadow: 0 12px 40px rgba(0,0,0,0.15);
    }
    
    .publication-links .button {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border: none;
      color: white;
      transition: all 0.3s ease;
      box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    
    .publication-links .button:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }
    
    .author-block a {
      color: #667eea;
      transition: color 0.3s ease;
    }
    
    .author-block a:hover {
      color: #764ba2;
      text-decoration: underline;
    }
    
    .title.is-3 {
      color: #2c3e50;
      font-weight: 600;
      margin-bottom: 2rem;
      position: relative;
      padding-bottom: 1rem;
    }
    
    .title.is-3::after {
      content: "";
      position: absolute;
      bottom: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 60px;
      height: 3px;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border-radius: 2px;
    }
    
    .content img {
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.1);
    }
    
    .content p {
      line-height: 1.8;
      color: #4a5568;
    }
    
    /* Method section styling */
    .method-box {
      background: linear-gradient(135deg, #f6f8fb 0%, #e9ecef 100%);
      border-left: 4px solid #667eea;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 0 8px 8px 0;
    }
    
    .method-box h4 {
      color: #667eea;
      margin-bottom: 1rem;
    }
    
    /* Footer styling */
    .footer {
      background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
      color: white;
      padding: 2rem 0;
    }
    
    .footer a {
      color: #667eea;
    }
    
    .footer a:hover {
      color: #764ba2;
    }
    
    /* BibTeX section */
    #BibTeX pre {
      background: #2c3e50;
      color: #ecf0f1;
      padding: 2rem;
      border-radius: 12px;
      box-shadow: 0 8px 32px rgba(0,0,0,0.2);
    }
    
    /* Case study styling */
    .case-study-box {
      position: relative;
      overflow: hidden;
    }
    
    .case-study-box::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 4px;
      background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    }
    
    /* Result cards enhancement */
    .results-carousel .box {
      min-height: 400px;
      display: flex;
      flex-direction: column;
      justify-content: center;
    }
    
    .results-carousel .box p {
      margin-top: 1.5rem;
      padding: 1rem;
      background: rgba(102, 126, 234, 0.05);
      border-radius: 8px;
      border-left: 3px solid #667eea;
    }
</style>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="title is-1 publication-title is-bold" style="display: flex; align-items: center; justify-content: center; margin-bottom: 2rem;">
              <svg width="640" height="220" viewBox="0 0 800 280" xmlns="http://www.w3.org/2000/svg" style="margin-right: 0;">
                <defs>
                  <linearGradient id="blueGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                    <stop offset="0%" style="stop-color:#1E90FF;stop-opacity:1" />
                    <stop offset="33%" style="stop-color:#4169E1;stop-opacity:1" />
                    <stop offset="66%" style="stop-color:#6495ED;stop-opacity:1" />
                    <stop offset="100%" style="stop-color:#87CEEB;stop-opacity:1" />
                  </linearGradient>
                  <linearGradient id="blueGradient_sec" x1="0%" y1="0%" x2="100%" y2="0%">
                    <stop offset="0%" style="stop-color:#74B9FF;stop-opacity:1" />
                    <stop offset="100%" style="stop-color:#0984E3;stop-opacity:1" />
                  </linearGradient>
                  <filter id="blue_shadow">
                    <feDropShadow dx="4" dy="4" stdDeviation="6" flood-opacity="0.4"/>
                  </filter>
                </defs>
                
                <!-- ARIA文字 - 更大更突出 -->
                <text x="400" y="140" font-family="Inter, Arial, sans-serif" font-size="108" font-weight="bold" fill="url(#blueGradient)" filter="url(#blue_shadow)" text-anchor="middle">ARIA</text>
                
                <!-- 右边的装饰元素 - 放大 -->
                <g class="floating-decorations">
                  <circle cx="680" cy="50" r="8" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                  <circle cx="720" cy="80" r="6" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.5"/>
                  <circle cx="700" cy="110" r="5" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.7"/>
                  <circle cx="750" cy="40" r="4" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.4"/>
                  <circle cx="730" cy="130" r="3" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                  <circle cx="760" cy="100" r="7" fill="url(#blueGradient)" class="floating-dot pulsing-dot" opacity="0.5"/>
                  <circle cx="690" cy="140" r="5" fill="url(#blueGradient_sec)" class="floating-dot pulsing-dot" opacity="0.6"/>
                </g>
                
                <!-- 副标题 -->
                <text x="400" y="180" font-family="Inter, Arial, sans-serif" font-size="20" font-weight="500" fill="url(#blueGradient)" opacity="0.8" text-anchor="middle">INTENTION-DRIVEN REWARD AGGREGATION</text>
              </svg>
            </div>
            <h2 class="subtitle is-2 publication-subtitle" style="font-size: 2rem; margin-top: 1rem;">
              Training Language Agents with Intention-Driven Reward Aggregation
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=asTSVwQAAAAJ&hl=zh-CN">Ruihan Yang*</a><sup style="color:#6fbf73;">1</sup>,</span>
              <span class="author-block">
                <a href="https://ykzhang721.github.io/">Yikai Zhang*</a><sup style="color:#6fbf73;">1†</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=FAJzMAQAAAAJ&hl=zh-CN">Aili Chen</a><sup style="color:#6fbf73;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://neph0s.github.io/">Xintao Wang</a><sup style="color:#6fbf73">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://siyuyuan.github.io/">Siyu Yuan</a><sup style="color:#6fbf73">1</sup>,
              </span>
              <br>
              <span class="author-block">
                <a href="https://jiangjiechen.github.io/">Jiangjie Chen</a><sup style="color:#ed4b82;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=uZdQxkwAAAAJ&hl=zh-CN">Deqing Yang</a><sup style="color:#6fbf73;">1†</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=odFW4FoAAAAJ">Yanghua Xiao</a><sup style="color:#6fbf73;">1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>Fudan University</span>
              <span class="author-block"><sup style="color:#ed4b82">2</sup>Bytedance Seed</span><br>
              <span class="author-block">* Equal Contribution</span><br>
              <span class="author-block">† Corresponding author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="static/papers/aria_paper.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/rhyang2021/ARIA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">🤗</p>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Twitter Link. -->
                <span class="link-block">
                  <a href="#"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-x-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <div class="box">
          <img src="static/images/aria_overview.png" alt="ARIA Overview" width="100%" />
          <p style="margin-top: 1rem; font-style: italic; color: #4a5568;">Overview of ARIA. ARIA first lets agents interact to collect trajectories, then performs semantic projection and aggregates rewards in the intention space, and finally updates the policy using the aggregated rewards.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">Introduction</h1>
    </div>
  </section>
  
  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="box">
            <div class="content has-text-justified">
              <p>
                Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL).
              </p>
              <p>
                To address this, we propose <strong>ARIA</strong>, a method that <strong>A</strong>ggregates <strong>R</strong>ewards in <strong>I</strong>ntention space to enable efficient and effective language <strong>A</strong>gents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization.
              </p>
              <p>
                Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average <strong>9.95%</strong> across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- METHOD SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">Method</h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Reward Aggregation</h2>
          <div class="box">
            <div class="content has-text-justified">
              <div class="method-box">
                <h4>Semantic Projection and Reward Aggregation</h4>
                <p>
                    Our approach leverages <strong>semantic projection</strong> to map different trajectories into clustered representations, enabling effective reward aggregation across semantically similar experiences.
                </p>
                <p>
                    <strong>Semantic Clustering:</strong> Each action and observation is embedded and clustered, producing cluster labels \(c_k(a_t)\) and \(c_k(o_t)\). This creates a clustered trajectory representation \(\tilde{h}_t = \{c_k(a_1), c_k(o_1), \ldots, c_k(a_{t-1}), c_k(o_{t-1})\}\).
                </p>
                <p>
                    <strong>Reward Aggregation:</strong> For each clustered state-action pair \((\tilde{h}_t, \tilde{a}_t)\), we aggregate rewards from all trajectories sharing this semantic pattern:
                </p>

                <div class="has-text-centered" style="padding: 1.5rem; background: rgba(102, 126, 234, 0.05); border-radius: 8px; margin: 1rem 0;">
                    $$\tilde{R}(\tilde{h}_t, \tilde{a}_t) = \frac{1}{|\mathcal{D}_{\tilde{h}_t, \tilde{a}_t}|} \sum_{\tau \in \mathcal{D}_{\tilde{h}_t, \tilde{a}_t}} \gamma^{T-t} R$$
                </div>

                <p>
                    where \(\mathcal{D}_{\tilde{h}_t, \tilde{a}_t}\) contains all trajectories with the same clustered prefix. This semantic-based aggregation pools reward signals from similar experiences, providing more robust value estimates and improving sample efficiency.
                </p>
                
                <p>
                    The aggregated reward \(\tilde{R}(\tilde{h}_t, \tilde{a}_t)\) is then assigned back to the original state-action pair \((h_t, a_t)\) as advantage \(\tilde{A}(h_t, a_t)\) for policy optimization.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Reward-Oriented Granularity Selection For Clustering</h2>
          <div class="box">
            <div class="content has-text-justified">
              <p>
                Traditional clustering metrics tend to favor overly coarse groupings due to high similarity among actions, overlooking fine-grained distinctions critical for RL tasks. We propose a reward-oriented granularity selection mechanism that assesses whether further splitting clusters yields meaningful reward change.
              </p>
              <p>
                <strong>SplitScore:</strong> Let $k \in [2, K]$ denote all possible granularity levels. We use SplitScore to select the optimal granularity $k^{*}$, defined as:
              </p>
              <div class="has-text-centered" style="padding: 1.5rem; background: rgba(102, 126, 234, 0.05); border-radius: 8px; margin: 1rem 0;">
                $$\text{SplitScore}(k) = \frac{\delta_k}{|\mathcal{D}|}$$
              </div>
              <p>
                where $\delta_k = \left| \tilde{R}^{(k+1)}(h_t, a_t) - \tilde{R}^{(k)}(h_t, a_t) \right|$ represents the reward change when the number of clusters changes from $k$ to $k+1$, and $\mathcal{D}$ is the collection of all $(h_t, a_t)$ pairs.
              </p>
              <p>
                <strong>Automatic Stopping Criterion:</strong> Given a threshold $\epsilon > 0$ and window size $\tau$, we stop splitting when $\text{SplitScore}(j) < \epsilon$ for all $j \in [k, k + \tau]$ as $k$ increases. The selected $k$ is then taken as $k^*$.
              </p>
            </div>
          </div>
        </div>
      </div>
    
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Offline REINFORCE with Aggregated Reward</h2>
          <div class="box">
            <div class="content has-text-justified">
              <p>
                We use the offline REINFORCE algorithm to optimize the policy. Formally, let $\pi_\theta(a \mid s)$ denote the policy parameterized by $\theta$ and assign the aggregated reward $\tilde{R}^{(k)}(h_t,a_t)$ to $\tilde{A}(h_t,a_t)$. ARIA optimizes the model by maximizing the following objective:
              </p>
              <div class="has-text-centered" style="padding: 1.5rem; background: rgba(102, 126, 234, 0.05); border-radius: 8px; margin: 1rem 0;">
                $$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T  \log \pi_\theta(a_t \mid h_t) \cdot \tilde{A}(h_t,a_t) \right]$$
              </div>
              <p>
                This objective leverages the variance-reduced advantages obtained through intention-aware reward aggregation, enabling more stable and efficient policy updates compared to standard REINFORCE with sparse rewards.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mathvista">Experimental Results</h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Main Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
        
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main_results_adversarial.png" alt="Main Results Adversarial" height="12%" />
                <p>Main results on adversarial games. ARIA achieves significant improvements of 9.67% and 9.83% on Bargaining and Negotiation tasks respectively, consistently outperforming strong offline and online RL baselines.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/main_results_single.png" alt="Main Results Single" width="45%" />
                <p>
                  Main results on single-agent games. ARIA outperforms all baselines by an average of 9.82% on Twenty Questions and Guess My City tasks, demonstrating its effectiveness across different task types.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/online_results.png" alt="Online Results" width="72%" />
                <p><strong>Online ARIA extends our method to dynamic learning scenarios.</strong> We first perform reward aggregation on pre-collected trajectories to initialize a reward model (RM) using Llama-3.1-8B-Instruct. The policy then generates new samples scored by the RM for policy updates, while the RM itself is periodically updated with fresh data, creating a co-evolving system. This online adaptation shows faster convergence and achieves higher returns compared to existing online methods (ArCHer and RAGEN) across iterations, demonstrating ARIA's effectiveness in both offline and online settings.</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Analysis</h2>
          <div id="analysis-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/reward_variance.png" alt="Reward Aggregation Significantly Reduces Reward Variance" width="78%" />
                <p><strong>Reward Aggregation Significantly Reduces Reward Variance:</strong> Our analysis demonstrates that reward aggregation markedly reduces the fluctuation range of action rewards compared to binary reward distributions that are highly polarized with values concentrated near 0 or 1. In large action spaces where most actions are sampled only once, the original binary rewards result in high variance. Through reward aggregation, actions within the same semantic cluster share a common reward, which significantly smooths the distribution and reduces variance across all tasks, highlighting the effectiveness and necessity of this approach in stabilizing policy learning.
                </p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/optimize_policy.png" alt="Reward Aggregation Improves Policy Optimization" width="78%" />
                <p><strong>Reward Aggregation Improves Policy Optimization:</strong> The policy loss curves reveal that ARIA, which applies semantic-level reward aggregation, accelerates loss reduction compared to vanilla REINFORCE baseline, providing stronger learning signals and enabling faster policy updates with improved sample efficiency. Despite converging to similar loss levels, ARIA outperforms other variants by 17.91% and 13.80% on bargaining and negotiation tasks respectively. We attribute these gains to the complementary effects of reward decay (introducing temporal structure for credit assignment) and reward aggregation (substantially lowering reward variance through shared signals for semantically similar actions), resulting in improved gradient estimation quality.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/generalization_results.png" alt="Generalization to Other Models" width="55%" />
                <p><strong>Generalization to Other Models:</strong> To assess the transferability of ARIA, we applied it to Qwen models (Qwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct) and observed consistent improvements across different base models. This demonstrates that our reward aggregation approach is model-agnostic and independent of specific architectural features or pretraining data of the underlying language models. We attribute this generalizability to the shared structural properties in the semantic spaces learned by large-scale language models, allowing ARIA to leverage these commonalities while preserving task-specific discriminative signals.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- THEORETICAL ANALYSIS SECTION -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Theoretical Analysis</h2>
          <div class="box">
            <div class="content has-text-justified">
              <p>
                We provide theoretical analysis showing that intention clustering-based aggregation reduces the variance of gradient descent while maintaining a small bound of bias, thus improving training stability and efficiency.
              </p>
              <p>
                <strong>Variance Reduction:</strong> By replacing original advantages $A$ with cluster-averaged advantages $\tilde{A}$, we remove the intra-cluster variance $\mathbb{E}[\text{Var}(A | C)]$, lowering the total variance of the policy gradient estimate: $\text{Var}(\tilde{A}) \leq \text{Var}(A)$.
              </p>
              <p>
                <strong>Bounded Bias:</strong> Through $\epsilon$-bisimulation analysis, we show that the bias introduced by reward aggregation is bounded: $|\mathbb{E}[\nabla_\theta \log \pi_\theta(a | h)(A(h, a) - \tilde{A}(h, a))]| \leq O(\epsilon)$, where actions within each cluster are $\epsilon$-bisimilar.
              </p>
              <p>
                <strong>Convergence Improvement:</strong> The variance reduction leads to faster convergence with fewer samples: $\|\hat{g} - g\|_2 = O(\sqrt{\sigma/N})$, where $\sigma$ is reduced through clustering.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- CASE STUDY SECTION -->
  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Case Studies</h2>
          <div id="case-carousel" class="carousel results-carousel">
            <div class="box m-5 case-study-box">
              <div class="content has-text-left">
                <h4><strong>Twenty Questions Example</strong></h4>
                <pre><code>Actor: Is it a living thing? Oracle: Yes.
Actor: Is it a mammal? Oracle: Yes.
Actor: Is it a human? Oracle: No.
Actor: Is it a carnivore? Oracle: No.
Actor: Is it a cow? Oracle: Yes.</code></pre>
                <p style="text-align: center;">Agent demonstrates strategic negotiation skills considering game dynamics and inflation effects.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BIBTEX SECTION -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@article{yang2024aria,
  title={ARIA: Training Language Agents with Intention-Driven Reward Aggregation},
  author={Yang, Ruihan and Zhang, Yikai and Chen, Aili and Wang, Xintao and Yuan, Siyu and Chen, Jiangjie and Yang, Deqing and Xiao, Yanghua},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a
              rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>